# Azure Retail Pipeline

[![CI/CD](https://github.com/igobymatthew/Azure_Retail_Pipeline/actions/workflows/deploy.yml/badge.svg)](https://github.com/igobymatthew/Azure_Retail_Pipeline/actions)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

> **Status:** In Development  
> A demonstration project for Azure data pipeline skills using ADF, Azure SQL, Databricks, SSIS, and Python.

---

## ğŸ“‘ Table of Contents
- [Overview](#overview)
- [Architecture](#architecture)
- [Technologies Used](#technologies-used)
- [Folder Structure](#folder-structure)
- [Setup Instructions](#setup-instructions)
- [Future Enhancements](#future-enhancements)

---

## ğŸ“Œ Overview
This project simulates a retail business scenario by implementing an end-to-end Azure data pipeline using:
- Azure Data Factory (ADF) for orchestration
- Azure SQL Database for centralized storage
- Databricks for transformations
- Python for business logic and anomaly detection
- SSIS for legacy data integration (optional)

---

## ğŸ§± Architecture
**Data Flow:**

```
CSV / Legacy SQL â†’ ADF / SSIS â†’ Azure SQL DB â†’ Databricks â†’ Azure SQL Reporting â†’ Power BI (optional)
```

---

## ğŸ› ï¸ Technologies Used
- **Azure Data Factory (ADF)**
- **Azure SQL Database**
- **Azure Databricks (PySpark)**
- **SSIS**
- **Python (pandas)**
- **GitHub Actions (CI/CD)**

---

## ğŸ“ Folder Structure

```text
azure-retail-pipeline/
â”œâ”€â”€ data/         # Sample CSV files
â”œâ”€â”€ notebooks/    # Databricks transformation scripts
â”œâ”€â”€ python/       # ETL and analytics scripts (Python)
â”œâ”€â”€ ssis/         # SSIS package or notes
â”œâ”€â”€ adf/          # ADF pipeline definitions (JSON)
â”œâ”€â”€ sql/          # SQL schema and scripts
â”œâ”€â”€ diagrams/     # Architecture diagrams and flowcharts
â””â”€â”€ README.md     # Project documentation
```

---

## ğŸš€ Setup Instructions

### Prerequisites
- Azure Subscription
- Azure SQL Database + logical server
- Azure Data Factory
- Azure Databricks workspace

### Step-by-Step
1. Deploy the SQL schema using `sql/create_sales_table.sql`
2. Import ADF pipeline JSON via Azure Portal or CLI
3. Upload `notebooks/sales_transform.py` to Databricks
4. Run `python/detect_anomalies.py` locally or via automation

---

## ğŸ”® Future Enhancements
- Power BI dashboard integration
- Key Vault for secure credentials
- CI/CD pipeline for ADF, SQL, Python, and Databricks
- Terraform/Bicep infrastructure automation

---

## ğŸ“„ License
MIT â€” feel free to fork and adapt.

